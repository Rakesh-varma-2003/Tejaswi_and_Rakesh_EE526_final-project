# Tejaswi_and_Rakesh_EE526_final-project

Our project focuses on the educational implementation of Knowledge Distillation (KD) for compressing deep learning models. This repository demonstrates teacherâ€“student training, soft-label supervision using temperature scaling, and student model optimization, developed as part of an EE526 course project.

This repository contains an end-to-end, educational implementation of Knowledge Distillation. The project illustrates teacher model training, baseline student training, and distillation-based student learning using a combined cross-entropy and KL-divergence loss. Experiments are conducted on the CIFAR-10 dataset to demonstrate how knowledge distillation enhances student model accuracy without increasing model size or inference cost, aligning with concepts from EE526, such as loss design, probabilistic outputs, and gradient-based optimization.
